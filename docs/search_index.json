[["index.html", "NYC Crime Chapter 1 Introduction", " NYC Crime Group 32: Abhiram Gaddam, Faizan Dogar, Devan Samant 2022-12-13 Chapter 1 Introduction Have you ever gotten out of a class at 9:30pm and wondered if it was safe to walk home? Do you wonder if you should cut through a park or take the subway? Is it possible to quantify “safe”? Does your risk profile change after 10:30pm or if you are a certain gender? The motivation for this project spawned from these practical questions we had in relation to navigating NYC on and off campus. Many of our classes get out at night and we live nearby but wonder if there is a walking path home that exhibits the least risk. We want to answer these types of questions using data and visualizations. The focus of this project is to analyze NYPD crime and complaint data around Columbia during the school year. However, the data (described more in the next section) lends itself to investigate broader questions of equity by demographics and neighborhood across NYC. "],["proposal.html", "Chapter 2 Proposal 2.1 Research topic 2.2 Data availability", " Chapter 2 Proposal 2.1 Research topic Have you ever gotten out of a class at 9:30pm and wondered if it was safe to walk home? Do you wonder if you should cut through a park or take the subway? Is it possible to quantify “safe”? Does your risk profile change after 10:30pm or if you are a certain gender? The motivation for this project spawned from these practical questions we had in relation to navigating NYC on and off campus. Many of our classes get out at night and we live nearby but wonder if there is a walking path home that exhibits the least risk. We want to answer these types of questions using data and visualizations. The focus of this project is to analyze NYPD crime and complaint data around Columbia during the school year. However, the data (described more in the next section) lends itself to investigate broader questions of equity by demographics and neighborhood across NYC. We also have flexibility in the time range to investigate and may look at longer trends. 2.2 Data availability NYC Open Data (NYC Office of Technology and Innovation (OTI)) in conjunction with New York City Police Department (NYPD) makes public safety data available for anyone online. In particular they publish Complaint Data which contains felony, misdemeanor, and violation crimes reported to the NYPD from 2006 till present. Year-to-Date (YTD): https://data.cityofnewyork.us/Public-Safety/NYPD-Complaint-Data-Current-Year-To-Date-/5uac-w243 Historic: https://data.cityofnewyork.us/Public-Safety/NYPD-Complaint-Data-Historic/qgea-i56i 2.2.1 Content “Complaint data” is a mixed set of records because it contains crimes (ex. robbery, rape, assault), parking violations, complaints of harassment, reports of abandoned animals, and more. We will need to investigate, clean, and filter the data to answer our research questions. As the links above indicate, there are two sets of data containing distinct records. A year-to-date dataset from 1/1/2022 to 9/30/2022 and a historic dataset containing records from 1/1/2006 to 12/31/2021. We plan to start with the year-to-date data and potentially incorporate historic records as needed. The dataset contains mostly categorical variables and dates with each row indicating a crime/violation. As of the last update on October 19, 2022, the YTD Dataset contains 397K rows and 36 columns. A data dictionary is provided by NYC Open Data at the link above. Column names and description: S.No Column Name (Data Type): Description CMPLNT_NUM (text): Randomly generated persistent ID for each complaint ADDR_PCT_CD (text): The precinct in which the incident occurred BORO_NM (text): The name of the borough in which the incident occurred CMPLNT_FR_DT (DateTime): Exact date of occurrence for the reported event (or starting date of occurrence, if CMPLNT_TO_DT exists) CMPLNT_FR_TM (text): Exact time of occurrence for the reported event (or starting time of occurrence, if CMPLNT_TO_TM exists) CMPLNT_TO_DT (DateTime): Ending date of occurrence for the reported event, if exact time of occurrence is unknown CMPLNT_TO_TM (text): Ending time of occurrence for the reported event, if exact time of occurrence is unknown CRM_ATPT_CPTD_CD (text): Indicator of whether crime was successfully completed or attempted, but failed or was interrupted prematurely HADEVELOPT (text): Name of NYCHA housing development of occurrence, if applicable HOUSING_PSA (Number): Development Level CodeNumber JURISDICTION_CODE (Number): Jurisdiction responsible for incident. Either internal, like Police(0), Transit(1), and Housing(2); or external(3), like Correction, Port Authority, etc. JURIS_DESC (text): Description of the jurisdiction code KY_CD (Number): Three digit offense classification code LAW_CAT_CD (text): Level of offense: felony, misdemeanor, violation LOC_OF_OCCUR_DESC (text): Specific location of occurrence in or around the premises; inside, opposite of, front of, rear of OFNS_DESC (text): Description of offense corresponding with key code PARKS_NM (text): Name of NYC park, playground or greenspace of occurrence, if applicable (state parks are not included) PATROL_BORO (text): The name of the patrol borough in which the incident occurred PD_CD (Number): Three digit internal classification code (more granular than Key Code) PD_DESC (text): Description of internal classification corresponding with PD code (more granular than Offense Description) PREM_TYP_DESC (text): Specific description of premises; grocery store, residence, street, etc. RPT_DT (DateTime): Date event was reported to police STATION_NAME (text): Transit station name SUSP_AGE_GROUP (text): Suspect’s Age Group SUSP_RACE (text): Suspect’s Race Description SUSP_SEX (text): Suspect’s Sex Description TRANSIT_DISTRICT (Number): Transit district in which the offense occurred VIC_AGE_GROUP (text): Victim’s Age Group VIC_RACE (text): Victim’s Race Description VIC_SEX (text): Victim’s Sex Description X_COORD_CD (Number): X-coordinate for New York State Plane Coordinate System, Long Island Zone, NAD 83, units feet (FIPS 3104) Y_COORD_CD (Number): Y-coordinate for New York State Plane Coordinate System, Long Island Zone, NAD 83, units feet (FIPS 3104) Latitude (Number): Midblock Latitude coordinate for Global Coordinate System, WGS 1984, decimal degrees (EPSG 4326) Longitude (Number): Midblock Longitude coordinate for Global Coordinate System, WGS 1984, decimal degrees (EPSG 4326) LatLon (Location) New Georeferenced Column (Point) 2.2.2 Who Collects The Data The data was collected and published by the NY Police Department (NYPD). 2.2.3 Format and Importation OTI provides the data in 4 formats: 1) online table, 2) visualization, 3) CSV, and 4) API. The first two have limited functionality and customization so we will not use those versions. The API requires an account and authentication tokens. Given that the CSV for YTD data is not too large (140MB), it seems the easiest to work with. CSV is a format we are most familiar with and one that R handles well. Is there a difference between CSV and API? Yes. Not in number of observations but in the columns. With the CSV, we get a total of 36 columns (listed above) but with the API we get a total of 41 columns. Following are the columns that are present in the CSV data but not in the API data: New Georeferenced Column. Following are the columns that are present in the API data but not in the CSV data: :@computed_region_92fq_4b7q :@computed_region_yeji_bk3q :@computed_region_efsh_h5xi geocoded_column :@computed_region_sbqj_enih :@computed_region_f5dn_yrer 2.2.4 Updates This dataset was first made public on 11/1/2018. It is updated quarterly. It appears that the metadata is updated more frequently but it is not clear what changes are incorporated as there could be several updates made in Oct 2022 but the data will only contain records up to the prior month end of Sept 2022. 2.2.5 Expected Challenges The data does not have many continuous variables. We will have to derive one based on counts and may need to supplement with other data to obtain others (ex. average cost/insurance per type of crime). There are longitude and latitude fields which we would like to utilize but have not identified an R package to process and visualize. "],["data.html", "Chapter 3 Data 3.1 Source 3.2 Data Transformation and Cleaning 3.3 Filter Table for Relevance 3.4 Select and Rename Columns 3.5 Missing Value Analysis", " Chapter 3 Data library(tidyverse) library(dplyr) library(lubridate) library(geosphere) library(redav) #remotes::install_github(&quot;jtr13/redav&quot;) library(ggridges) library(ggplot2) library(scales) library(forcats) library(Lock5withR) library(tidyr) library(vcdExtra) library(ggmap) # D. Kahle and H. Wickham. ggmap: Spatial Visualization with ggplot2. The R Journal, 5(1), 144-161. URL http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf library(RColorBrewer) 3.1 Source As discussed in the Proposal section, the data for this project comes from published crime reports. NYC Open Data (NYC Office of Technology and Innovation (OTI)) in conjunction with New York City Police Department (NYPD) makes public safety data available for anyone online. In particular they publish Complaint Data which contains felony, misdemeanor, and violation crimes reported to the NYPD from 2006 till present. Year-to-Date (YTD) (2022-01 to 2022-09): https://data.cityofnewyork.us/Public-Safety/NYPD-Complaint-Data-Current-Year-To-Date-/5uac-w243 The dataset contains mostly categorical variables and dates with each row indicating a crime/violation. As of the last update on October 19, 2022, the YTD Dataset contains 397K rows and 36 columns. A data dictionary is provided by NYC Open Data at the link above. Additional details can be found in the Proposal: https://ds4114.github.io/NYC_Crime_Group32/proposal.html#data-availability 3.2 Data Transformation and Cleaning This .Rmd file contains the following sections: -Data Loading: To show data was ingested into the environment -Bucketing: To combine values in a derived field for simplifying graphs -Date Transformations: To make strings into dates and extract time information -Derived Geo Fields: To calculate distances between points -Derive Other Fields: To derived other fields and perform minor cleaning operations We then filter our data frame to relevant records, and then select key fields for ease-of-use. “Relevant” records are defined by time frame and crime category explained more below. There is also a section at the end on missing data. 3.2.1 Data Loading #Group32 - This file has been added to gitignore so it will not be uploaded. So we are on the same page and can run the same code, add the csv to your local project folder in a new subfolder &quot;data&quot;. df_raw &lt;- read_csv(&#39;./data/NYPD_Complaint_Data_Current__Year_To_Date_.csv&#39; , col_types= cols( CMPLNT_NUM = col_character() #was loading as number so numbers with letters were showing as null ) ) #View(df_raw) #head(df_raw) #make a copy of the raw data to manipulate df &lt;- df_raw 3.2.2 Bucket Crime Types This code creates a new field to simplify the crime category to reduce the number of unique values. df &lt;- df %&gt;% mutate(CRIME_CAT = case_when( OFNS_DESC %in% c(&quot;RAPE&quot;, &quot;SEX CRIMES&quot;, &quot;HARRASSMENT 2&quot;, &quot;FELONY SEX CRIMES&quot;, &quot;PROSTITUTION &amp; RELATED OFFENSES&quot;) ~ &quot;SEX CRIMES&quot;, OFNS_DESC %in% c(&quot;DANGEROUS DRUGS&quot;,&quot;CANNABIS RELATED OFFENSES&quot;,&quot;INTOXICATED &amp; IMPAIRED DRIVING&quot;,&quot;ALCOHOLIC BEVERAGE CONTROL LAW&quot;,&quot;INTOXICATED/IMPAIRED DRIVING&quot;) ~ &quot;DRUG AND ALCOHOL RELATED&quot;, OFNS_DESC %in% c(&quot;ROBBERY&quot;,&quot;GRAND LARCENY&quot;,&quot;THEFT-FRAUD&quot;,&quot;PETIT LARCENY&quot;,&quot;BURGLARY&quot;,&quot;GRAND LARCENY OF MOTOR VEHICLE&quot;,&quot;POSSESSION OF STOLEN PROPERTY&quot;,&quot;THEFT OF SERVICES&quot;,&quot;BURGLAR&#39;S TOOLS&quot;,&quot;PETIT LARCENY OF MOTOR VEHICLE&quot;,&quot;OTHER OFFENSES RELATED TO THEF&quot;) ~ &quot;THEFT OR BURGLARY&quot;, OFNS_DESC %in% c(&quot;DANGEROUS WEAPONS&quot;,&quot;MURDER &amp; NON-NEGL. MANSLAUGHTER&quot;,&quot;KIDNAPPING &amp; RELATED OFFENSES&quot;,&quot;HOMICIDE-NEGLIGENT,UNCLASSIFIE&quot;,&quot;HOMICIDE-NEGLIGENT-VEHICLE&quot;,&quot;KIDNAPPING&quot;,&quot;FELONY ASSAULT&quot;,&quot;ARSON&quot;,&quot;ASSAULT 3 &amp; RELATED OFFENSES&quot;,&quot;UNLAWFUL POSS. WEAP. ON SCHOOL&quot;,&quot;MURDER &amp; NON-NEGL. MANSLAUGHTER&quot;) ~ &quot;MAJOR VIOLENT CRIMES&quot;, OFNS_DESC %in% c(&quot;CRIMINAL MISCHIEF &amp; RELATED OF&quot;,&quot;UNAUTHORIZED USE OF A VEHICLE&quot;,&quot;FRAUDS&quot;,&quot;OFFENSES AGAINST PUBLIC SAFETY&quot;,&quot;DISORDERLY CONDUCT&quot;,&quot;JOSTLING&quot;,&quot;DISRUPTION OF A RELIGIOUS SERV&quot;,&quot;ESCAPE 3&quot;,&quot;OFF. AGNST PUB ORD SENSBLTY &amp;&quot;,&quot;CRIMINAL TRESPASS&quot;,&quot;VEHICLE AND TRAFFIC LAWS&quot;,&quot;GAMBLING&quot;,&quot;OFFENSES AGAINST THE PERSON&quot;,&quot;OFFENSES INVOLVING FRAUD&quot;,&quot;FRAUDULENT ACCOSTING&quot;,&quot;ANTICIPATORY OFFENSES&quot;,&quot;LOITERING/GAMBLING (CARDS, DIC&quot;) ~ &quot;FRAUD/GAMBLING AND MISC&quot;, OFNS_DESC %in% c(&quot;NYS LAWS-UNCLASSIFIED FELONY&quot;,&quot;MISCELLANEOUS PENAL LAW&quot;,&quot;FORGERY&quot;,&quot;OFFENSES AGAINST PUBLIC ADMINI&quot;,&quot;CHILD ABANDONMENT/NON SUPPORT&quot;,&quot;NYS LAWS-UNCLASSIFIED VIOLATION&quot;,&quot;OTHER STATE LAWS&quot;,&quot;OTHER STATE LAWS (NON PENAL LAW)&quot;,&quot;NEW YORK CITY HEALTH CODE&quot;,&quot;ADMINISTRATIVE CODE&quot;,&quot;OTHER STATE LAWS (NON PENAL LA&quot;,&quot;AGRICULTURE &amp; MRKTS LAW-UNCLASSIFIED&quot;,&quot;ENDAN WELFARE INCOMP&quot;,&quot;OFFENSES RELATED TO CHILDREN&quot;) ~&quot;OTHER&quot;)) There are 5 values with missing Offense category. Since they have valid PD_CD (Crime ID) and PD_DESC, we can impute these values from other columns with the same PD_CD. -Two values are for obscenity - 594 PD_CD (categorized as sex crimes) -One values for crime pos weap - 797 PD_CD (categorized as major violent crimes/dangerous weapons) -One value for “place false bomb” - 648 PD_CD (categorized as other) -One value for “noise” - 872 PD_CD (categorized as other) df$CRIME_CAT[df$CMPLNT_NUM %in% c(&quot;243170965&quot;, &quot;245874611&quot;)] &lt;- &quot;OTHER&quot; df$CRIME_CAT[df$CMPLNT_NUM %in% c(&quot;248613125&quot;, &quot;248290778&quot;)] &lt;- &quot;SEX CRIMES&quot; df$CRIME_CAT[df$CMPLNT_NUM %in% (&quot;246605653&quot;)] &lt;- &quot;MAJOR VIOLENT CRIMES&quot; #&quot;DANGEROUS WEAPONS&quot; #No Nulls now #View(df[is.na(df$CRIME_CAT),]) 3.2.3 Date/Time Transformation Per the data dictionary, there is both a “from_date” and a “to_date” when the exact time is unknown. There is also a “report date” for when the crime was reported. Using these fields in conjunction, we can derive a new clean field that is the assumed date of the incident. Assumptions: When there is a range, we will use the “from” date only because is populated well and will on average approximate the frequency of crime over time; when from_date is null, we will use the report_date (does not occur often, see Missing Data Analysis below). df &lt;- df %&gt;% mutate( #use from date and report date if null. If to_date then just use from date and we can argue it averages out since new reports will start as other end Incident_Date_raw = case_when (is.null(CMPLNT_FR_DT) ~ RPT_DT ,CMPLNT_FR_DT == &quot;(null)&quot; ~ RPT_DT ,TRUE ~ CMPLNT_FR_DT ) #flag if estimated (ie from date is null or to date is populated) ,Incident_Date_Estimated_Flag = case_when ( is.null(CMPLNT_FR_DT) ~ &#39;Y&#39; ,CMPLNT_FR_DT == &quot;(null)&quot; ~ &#39;Y&#39; ,!is.null(CMPLNT_TO_DT) ~ &#39;Y&#39; ,CMPLNT_TO_DT != &quot;(null)&quot; ~ &#39;Y&#39; ,TRUE ~ &#39;N&#39; ) ) #Convert to times df &lt;- df %&gt;% mutate( #creating date and time together for lubridate Incident_Date = as.Date(Incident_Date_raw, format = &#39;%m/%d/%Y&#39;) ,Incident_Datetime = as.POSIXct(paste(Incident_Date_raw,CMPLNT_FR_TM), format = &#39;%m/%d/%Y %H:%M:%S&#39;) ) %&gt;% mutate( Incident_HourTime = hour(Incident_Datetime) + minute(Incident_Datetime)/60 ,Incident_Month = month(Incident_Date) ,Incident_DayOfWeek = wday(Incident_Date, label = TRUE, abbr = TRUE) ) 3.2.4 Geo-Location Fields This section uses a new package to calcluate the distance between two points in Lat/Long format. Here we are finding the distance from each crime to the center of the main Columbia campus to use later. #Location of Columbia - hardcoded for calculation CU_Latitude = 40.807384 CU_Longitude = -73.963036 df$dist_to_CU &lt;- apply(df, 1, function(x)distm( c(x[which( colnames(df)==&quot;Longitude&quot;)],x[which(colnames(df)==&quot;Latitude&quot;)]) ,c(CU_Longitude,CU_Latitude) ,fun = distGeo) ) 3.2.5 Other Derivations In this section, we add additional fields for more classification (explained more in the Results section), reclassify “null” to Unknown in specific instances, shorten values, and combine information to simplify categories. #Update certain null fields. Setting race to null because there is already an &quot;Unknown&quot; category df$LOC_OF_OCCUR_DESC[df$LOC_OF_OCCUR_DESC==&quot;(null)&quot;]&lt;-NA df$BORO_NM[df$BORO_NM==&quot;(null)&quot;]&lt;-NA df$SUSP_RACE[df$SUSP_RACE==&quot;(null)&quot;]&lt;-&quot;UNKNOWN&quot; df$VIC_RACE[df$VIC_RACE==&quot;(null)&quot;]&lt;-&quot;UNKNOWN&quot; df$SUSP_SEX[df$SUSP_SEX==&quot;(null)&quot;]&lt;-&quot;U&quot; df &lt;- df %&gt;% mutate( #get a flag for outside vs inside Inside_Outside = case_when ( LOC_OF_OCCUR_DESC %in% c(&quot;FRONT OF&quot; , &quot;OPPOSITE OF&quot; , &quot;REAR OF&quot;) ~ &quot;OUTSIDE&quot; ,LOC_OF_OCCUR_DESC %in% c(&quot;INSIDE&quot;) ~ &quot;INSIDE&quot; #,LOC_OF_OCCUR_DESC == &quot;(null)&quot; ~ NULL #doesnt run so added statement above ,TRUE ~ LOC_OF_OCCUR_DESC ) #if victim was a person (not a business/govt) ,VIC_Individual_Flag = case_when ( VIC_SEX %in% c(&quot;M&quot;,&quot;F&quot;,&quot;L&quot;) ~ &#39;Y&#39; ,TRUE ~ &#39;N&#39; ) ,SUSP_AGE_GROUP = case_when ( SUSP_AGE_GROUP %in% c(&#39;&lt;18&#39;,&#39;18-24&#39;,&#39;25-44&#39;,&#39;45-64&#39;) ~ SUSP_AGE_GROUP ,TRUE ~ &#39;UNKNOWN&#39; ) ,VIC_AGE_GROUP = case_when ( VIC_AGE_GROUP %in% c(&#39;&lt;18&#39;,&#39;18-24&#39;,&#39;25-44&#39;,&#39;45-64&#39;) ~ VIC_AGE_GROUP ,TRUE ~ &#39;UNKNOWN&#39; ) ,SUSP_RACE_short = case_when ( SUSP_RACE == &#39;AMERICAN INDIAN/ALASKAN NATIVE&#39; ~ &#39;AI&#39; ,SUSP_RACE == &#39;ASIAN / PACIFIC ISLANDER&#39; ~ &#39;AP&#39; ,SUSP_RACE == &#39;BLACK&#39; ~ &#39;B&#39; ,SUSP_RACE == &#39;BLACK HISPANIC&#39; ~ &#39;BH&#39; ,SUSP_RACE == &#39;UNKNOWN&#39; ~ &#39;U&#39; ,SUSP_RACE == &#39;WHITE&#39; ~ &#39;W&#39; ,SUSP_RACE == &#39;WHITE HISPANIC&#39; ~ &#39;WH&#39; ) ,VIC_RACE_short = case_when ( VIC_RACE == &#39;AMERICAN INDIAN/ALASKAN NATIVE&#39; ~ &#39;AI&#39; ,VIC_RACE == &#39;ASIAN / PACIFIC ISLANDER&#39; ~ &#39;AP&#39; ,VIC_RACE == &#39;BLACK&#39; ~ &#39;B&#39; ,VIC_RACE == &#39;BLACK HISPANIC&#39; ~ &#39;BH&#39; ,VIC_RACE == &#39;UNKNOWN&#39; ~ &#39;U&#39; ,VIC_RACE == &#39;WHITE&#39; ~ &#39;W&#39; ,VIC_RACE == &#39;WHITE HISPANIC&#39; ~ &#39;WH&#39; ) ) %&gt;% mutate( Complaint_Count = 1 #maybe want to add like a intensity value or something? #TODO, make this field a little better/check values? ,Premise_Derived = case_when ( Inside_Outside == &#39;INSIDE&#39; ~ &#39;INSIDE&#39; ,PREM_TYP_DESC ==&quot;RESIDENCE - APT. HOUSE&quot; &amp; (Inside_Outside == &quot;(null)&quot; | is.null(Inside_Outside)) ~ &#39;INSIDE&#39; ,!is.null(PARKS_NM) &amp; PARKS_NM != &quot;(null)&quot; ~ &#39;PARK&#39; ,PREM_TYP_DESC %in% c(&quot;TRANSIT - NYC SUBWAY&quot;,&quot;BUS (NYC TRANSIT)&quot;,&quot;TRANSIT FACILITY (OTHER)&quot;) ~ &#39;SUBWAY&#39; ,TRUE ~ &#39;STREET&#39; ) ) %&gt;% mutate(Borough_short = case_when( BORO_NM %in% c(&quot;BRONX&quot;) ~ &quot;BX&quot;, BORO_NM %in% c(&quot;BROOKLYN&quot;) ~ &quot;BK&quot;, BORO_NM %in% c(&quot;MANHATTAN&quot;) ~ &quot;MH&quot;, BORO_NM %in% c(&quot;QUEENS&quot;) ~ &quot;QN&quot;, BORO_NM %in% c(&quot;STATEN ISLAND&quot;) ~ &quot;SI&quot;) ) 3.3 Filter Table for Relevance The raw data only contains crimes that were reported in 2022 even if they took place earlier. As we will show in the Results, this create a bias in the time series and this section creates a new data frame to filter out old records (as defined by derived field above; before 1/1/2022). We also filter for only “relevant” crimes - i.e. excluding parking violations, noise compliants (those in the “Other” category) df_filter &lt;- df %&gt;% filter( Incident_Date &gt;= as.Date(&#39;2022/01/01&#39;) ) %&gt;% filter ( !CRIME_CAT %in% c(&quot;OTHER&quot;) ) #TODO- distance filter? #View(df_filter) #unique(df_filter$CRIME_CAT) This improves the results as we will show in the next chapter. There are not many records that are excluded (396978 - 364298) 3.4 Select and Rename Columns This section simply chooses fields that are used in our analysis to simplify and reduce the overall size of the working data frame. #Not required df_key_fields &lt;- df_filter %&gt;% select( #Basic Info CMPLNT_NUM #,CRM_ATPT_CPTD_CD #,Complaint_Count #derived #Date Info ,Incident_Date_raw #derived #,Incident_Date_Estimated_Flag #derived #,Incident_Date #derived #,Incident_Datetime #derived #,Incident_HourTime #derived #,Incident_Month #derived #,Incident_DayOfWeek #derived ,CMPLNT_FR_DT ,CMPLNT_FR_TM #,CMPLNT_TO_DT #,CMPLNT_TO_TM ,RPT_DT #Location Info #,Inside_Outside #derived #missing a lot (because derived on field below) #,CU_Latitude #derived #,CU_Longitude #derived ,dist_to_CU #derived ,Premise_Derived #derived #,ADDR_PCT_CD ,BORO_NM #,HADEVELOPT #,HOUSING_PSA #,JURISDICTION_CODE ,JURIS_DESC #,LOC_OF_OCCUR_DESC #missing a lot (because of privacy) #,PARKS_NM #,PATROL_BORO #,PD_CD #,PD_DESC #,PREM_TYP_DESC #,STATION_NAME #,TRANSIT_DISTRICT #,X_COORD_CD #,Y_COORD_CD ,Latitude ,Longitude ,Lat_Lon #,`New Georeferenced Column` #Crime Info ,CRIME_CAT #derived ,VIC_Individual_Flag #derived #,KY_CD ,LAW_CAT_CD ,OFNS_DESC ,SUSP_AGE_GROUP ,SUSP_RACE ,SUSP_RACE_short ,SUSP_SEX ,VIC_AGE_GROUP ,VIC_RACE ,VIC_RACE_short ,VIC_SEX ) df_key_fields$FCT_CRIME_CAT &lt;- as.factor(df_key_fields$CRIME_CAT) df_key_fields$FCT_CRIME_CAT &lt;- fct_infreq(df_key_fields$CRIME_CAT) df_key_fields$VIC_SEX &lt;- as.factor(df_key_fields$VIC_SEX) df_key_fields$VIC_SEX &lt;- fct_relevel(df_key_fields$VIC_SEX, c(&#39;M&#39;,&#39;F&#39;,&#39;L&#39;,&#39;E&#39;,&#39;D&#39;)) df_key_fields$SUSP_SEX &lt;- as.factor(df_key_fields$SUSP_SEX) df_key_fields$SUSP_SEX &lt;- fct_relevel(df_key_fields$SUSP_SEX, c(&#39;M&#39;,&#39;F&#39;,&#39;L&#39;,&#39;E&#39;,&#39;D&#39;)) 3.5 Missing Value Analysis First let us look at the raw data to see what things look like originally. We can categorize nulls as real (i.e. true NA) vs “fake” (i.e. “null” in the data or similar). Then show which fields and what percent of each may have an issue. df_nulls &lt;- df %&gt;% #head(1000) %&gt;% mutate_all(as.character) %&gt;% pivot_longer(cols = !CMPLNT_NUM, names_to = &quot;field&quot;, values_to = &quot;value&quot;) %&gt;% mutate( Real_Null = case_when(is.na(value) ~ 1, TRUE ~ 0) ,Fake_Null = case_when( value == &quot;(null)&quot; ~ 1, TRUE ~ 0) ) %&gt;% filter( Real_Null + Fake_Null &gt;= 1) %&gt;% group_by(field) %&gt;% summarize(total_real_null=sum(Real_Null),total_fake_null=sum(Fake_Null), total_null=sum(Real_Null)+sum(Fake_Null)) %&gt;% mutate(total_null_percent = total_null/396978) %&gt;% arrange(desc(total_null)) print(df_nulls, n=50) ## # A tibble: 23 × 5 ## field total_real_null total_fake_null total_null total_n…¹ ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 HADEVELOPT 0 395756 395756 9.97e-1 ## 2 PARKS_NM 0 394295 394295 9.93e-1 ## 3 STATION_NAME 0 388677 388677 9.79e-1 ## 4 TRANSIT_DISTRICT 388677 0 388677 9.79e-1 ## 5 HOUSING_PSA 371512 0 371512 9.36e-1 ## 6 Inside_Outside 66081 0 66081 1.66e-1 ## 7 LOC_OF_OCCUR_DESC 66081 0 66081 1.66e-1 ## 8 CMPLNT_TO_DT 29393 0 29393 7.40e-2 ## 9 CMPLNT_TO_TM 0 29276 29276 7.37e-2 ## 10 BORO_NM 657 0 657 1.66e-3 ## 11 Borough_short 657 0 657 1.66e-3 ## 12 PREM_TYP_DESC 0 476 476 1.20e-3 ## 13 PD_CD 440 0 440 1.11e-3 ## 14 PD_DESC 0 440 440 1.11e-3 ## 15 ADDR_PCT_CD 20 0 20 5.04e-5 ## 16 Lat_Lon 9 0 9 2.27e-5 ## 17 Latitude 9 0 9 2.27e-5 ## 18 Longitude 9 0 9 2.27e-5 ## 19 New Georeferenced Column 9 0 9 2.27e-5 ## 20 X_COORD_CD 9 0 9 2.27e-5 ## 21 Y_COORD_CD 9 0 9 2.27e-5 ## 22 OFNS_DESC 0 5 5 1.26e-5 ## 23 PATROL_BORO 0 2 2 5.04e-6 ## # … with abbreviated variable name ¹​total_null_percent So we see we have mostly missing data for 5 fields. Location and its derived field is also missing a good portion which is likely due to masking for privacy reasons per the data source notes. Let us look at missing patterns now: #View(df_key_fields[is.na(df_key_fields$CMPLNT_NUM),]) #View(df[!is.na(df$TRANSIT_DISTRICT),]) plot_missing(df, percent = FALSE) plot_missing( df[ , !colnames(df) %in% c(&quot;TRANSIT_DISTRICT&quot;,&quot;HOUSING_PSA&quot;) ] , percent = FALSE) These charts are hard to read given the number of columns but it shows that most data is actually populated very well as we saw above. There are two problem fields causing all of the real nulls but we can re-graph this without those and see that complete cases are at the top. However, this is still not accurate because there are string values in the data that say “(null)” or “Unknown” which would appear as populated. In any case, we do not need to clean and evaluate all columns, so let us work off of the new, filtered dataset (see below). Side Note: We initially noticed the Complaint_Num was null often but this was due to it containing alphanumeric characters and R loading it as a number only. It has been corrected and serves as a primary key for the data frame. plot_missing(df_key_fields, percent = FALSE) The filtered data (relevant columns and rows only) is populated very well and this dataset accounts for improperly coded values where “(null)” will appear as NULL properly. We have recoded some values to “Unknown” as that appears in the raw data is is reasonable (e.g. if the suspect was not found, we have no information on them). We see there are a few blanks for Borough and lat/long and thus the related derived field, but otherwise we have a good data frame to use in the next chapter. "],["results.html", "Chapter 4 Results 4.1 Crime Category and Demographic Analysis 4.2 Time Series Analysis 4.3 Location Analysis 4.4 Various Other Plots (To Revise/Delete)", " Chapter 4 Results 4.1 Crime Category and Demographic Analysis We start our analysis of crime types by investigating what types of crimes occur in what frequencies. We bucketed the numerous crimes from the raw data into logical categories. Note that in the pre-processing stage, we removed a small “Other” category of parking violations and noise complaints. #moved factor releveling to data.Rmd df_counts &lt;- df_key_fields %&gt;% group_by(CRIME_CAT) %&gt;% summarize(total_count=n(), .groups = &#39;drop&#39;) %&gt;% as.data.frame() ggplot(df_counts) + geom_bar(aes(y=CRIME_CAT, x=total_count), stat=&quot;identity&quot;, fill = &quot;cornflowerblue&quot;) + ggtitle(&quot;Number of Reports Per Crime Category &quot;) + xlab(&quot;Count&quot;) + ylab(&quot;Category&quot;) + scale_x_continuous(label=comma) We observe that theft or burglary related crimes are the most common by far (more than 2x the next highest category). This category is followed by sex crimes, violent crimes, and fraud/gambling related crimes all of which are relatively close in number. The lowest category by far is drug and alcohol related crimes, perhaps because the policing of these crimes has been relaxed over the last few years. We can further investigate the data to see the top 15 kinds of crimes and what categories they belong to. df_key_fields %&gt;% group_by(OFNS_DESC,FCT_CRIME_CAT) %&gt;% summarize(freq = n()) %&gt;% arrange(desc(freq)) %&gt;% #filter(freq&gt;2700) %&gt;% head(15) %&gt;% ggplot(aes(x = freq, y = fct_reorder(OFNS_DESC, freq), color = FCT_CRIME_CAT)) + geom_segment(aes(yend = OFNS_DESC), xend = 0, colour = &quot;grey50&quot;) + geom_point(size = 3, aes(color = FCT_CRIME_CAT)) + ggtitle(&quot;Top 15 Most Commonly Reported Types of Crime&quot;) + xlab(&quot;Count&quot;) + ylab(&quot;Type of Crime&quot;) + labs(color = &quot;Aggregate Crime Category&quot;) + scale_x_continuous(label=comma) Here we can see the crime incident that has happened the most frequently is “Petit Larceny”, a form of larceny in which the value of the property taken is generally less than $50. As expected, 5 of the top 15 crimes are theft or burglary related, which explains why the most common crime category reported is theft. Also, we only see one incident type corresponding to drug and alcohol related crimes. 4.1.1 Race We can start our demographic analysis by digging further into the race and gender of both the suspects and victims. Let us begin by diving into trends between the race of the suspects and victims. df_key_fields %&gt;% group_by(SUSP_RACE, SUSP_RACE_short) %&gt;% summarize(freq = n()) %&gt;% arrange(desc(freq)) %&gt;% slice(1:7) %&gt;% ggplot(aes(x=fct_reorder(SUSP_RACE_short,freq,.desc = TRUE),y=freq)) + geom_bar(stat = &quot;identity&quot;,fill = &quot;cornflowerblue&quot;) + #scale_x_discrete(guide = guide_axis(n.dodge=3))+ ggtitle(&quot;Reports by Suspect Race&quot;) + xlab(&quot;Suspect Race&quot;) + ylab(&quot;Count&quot;) + labs(subtitle = &quot;U=Unknown, B=Black, WH=White/Hispanic, W=White, BH=Black/Hispanic, AP=Asian/Pacific Islander, AI=American Indian/Native&quot;) + scale_y_continuous(label=comma) df_key_fields %&gt;% group_by(VIC_RACE, VIC_RACE_short) %&gt;% summarize(freq = n()) %&gt;% arrange(desc(freq)) %&gt;% slice(1:6) %&gt;% ggplot(aes(x=fct_reorder(VIC_RACE_short,freq,.desc = TRUE),y=freq)) + geom_bar(stat = &quot;identity&quot;,fill = &quot;cornflowerblue&quot;) + #scale_x_discrete(guide = guide_axis(n.dodge=3))+ ggtitle(&quot;Reports by Victim Race&quot;) + xlab(&quot;Victim Race&quot;) + ylab(&quot;Count&quot;) + labs(subtitle = &quot;U=Unknown, B=Black, WH=White/Hispanic, W=White, BH=Black/Hispanic, AP=Asian/Pacific Islander, AI=American Indian/Native&quot;) + scale_y_continuous(label=comma) While there is a lot of null/unknown data, we notice that there are clear patterns between the race of the suspect and victim. Visualizing this using a heatmap convinces us of this. ggplot(df_key_fields, aes(VIC_RACE_short, SUSP_RACE_short)) + geom_bin2d()+ scale_fill_continuous(label=comma)+ #scale_x_discrete(guide = guide_axis(n.dodge=3)) + xlab(&quot;Suspect Race&quot;) + ylab(&quot;Victim Race&quot;) + ggtitle(&quot;Victim Race vs Suspect Race&quot;) + labs(subtitle = &quot;U=Unknown, B=Black, WH=White/Hispanic, W=White, BH=Black/Hispanic, AP=Asian/Pacific Islander, AI=American Indian/Native&quot;, fill=&quot;Count&quot;) We see from the bar charts that most of the victims are black and also that most of the suspects are black. Logically, the intersection of these two fields is also the highest. This is made evident in the heatmap. 4.1.2 Gender Next, lets do a gender focused analysis. df_key_fields %&gt;% group_by(SUSP_SEX) %&gt;% summarize(freq = n()) %&gt;% arrange(desc(freq)) %&gt;% #slice(1:6) %&gt;% ggplot(aes(x=fct_reorder(SUSP_SEX,freq,.desc = TRUE),y=freq)) + geom_bar(stat = &quot;identity&quot;,fill = &quot;cornflowerblue&quot;) + #scale_x_discrete(guide = guide_axis(n.dodge=3))+ labs( x =&quot;Suspect Sex&quot; ,y =&quot;Count&quot; ,title = &quot;Reports by Suspect Sex&quot; ,subtitle = &quot;M=Male, F=Female, L=Law Enforcement, E=Corporation, D=NY State&quot;, fill=&quot;Count&quot;) + scale_y_continuous(lim=c(0,175000), label=comma) df_key_fields %&gt;% group_by(VIC_SEX) %&gt;% summarize(freq = n()) %&gt;% arrange(desc(freq)) %&gt;% #slice(1:6) %&gt;% ggplot(aes(x=fct_reorder(VIC_SEX,freq,.desc = TRUE),y=freq)) + geom_bar(stat = &quot;identity&quot;,fill = &quot;cornflowerblue&quot;) + #scale_x_discrete(guide = guide_axis(n.dodge=3))+ labs( x =&quot;Victim Sex&quot; ,y =&quot;Count&quot; ,title = &quot;Reports by Victim Sex&quot; ,subtitle = &quot;M=Male, F=Female, L=Law Enforcement, E=Corporation, D=NY State&quot;, fill=&quot;Count&quot;) + scale_y_continuous(lim=c(0,175000), label=comma) Most of the victims are female, followed closely by males. However, there is a large discrepancy in the counts of suspect genders. Males are by far the highest category, more than 4x the count for females. ggplot(df_key_fields, aes(SUSP_SEX,VIC_SEX)) + geom_bin2d()+ #scale_x_discrete(guide = guide_axis(n.dodge=3)) + labs( x =&quot;Suspect Sex&quot; ,y =&quot;Victim Sex&quot; ,title = &quot;Victim Sex vs Suspect Sex&quot; ,subtitle = &quot;M=Male, F=Female, L=Law Enforcement, E=Corporation, D=NY State&quot;, fill=&quot;Count&quot;) –TODO, do we need 3 charts above that show the same thing? –Just the heatmap is sufficient I think (Abhi) 4.1.3 Crime by Borough When one decides to attend Columbia University, one may ask themselves which borough should they live in depending on the relative safety of living in each borough. Therefore, following are the plots of count of total crimes and different types of crimes by the boroughs. # Counts by Boroughs ggplot(df_filter) + geom_bar(aes(x=fct_infreq(Borough_short))) + labs( x =&quot;Borough&quot; ,y =&quot;Count&quot; ,title = &quot;Crime by Borough&quot; ,subtitle = &quot;BK=Brooklyn, BX=Bronx, MH=Manhattan, QN=Queens, SI=Staten Island&quot;)+ scale_y_continuous(label=comma) ggplot(df_filter) + geom_bar(aes(x=Borough_short)) + facet_wrap(~CRIME_CAT) + labs( x =&quot;Borough&quot; ,y =&quot;Count&quot; ,title = &quot;Crime by Borough and Type&quot; ,subtitle = &quot;BK=Brooklyn, BX=Bronx, MH=Manhattan, QN=Queens, SI=Staten Island&quot;)+ scale_y_continuous(label=comma) From these graphs, we note that with respect to overall crime, Brooklyn has the most crime followed by Manhattan. Secondly, it’s interesting to see that while Brooklyn has more crime of type “SEX CRIMES”, “MAJOR VIOLENT CRIMES”, AND “FRAUD/GAMBLING AND MISC” compared to Manhattan while Manhattan has more crime of type “THEFT OR BURGLARY” and “DRUG AND ALCOHOL RELATED” compared to Brooklyn. Let us now take a look at other variables in the dataset. After some trial and error, the following yields some insights (looking at only crimes against individuals): counts3 &lt;- df_filter %&gt;% filter(VIC_Individual_Flag == &quot;Y&quot;) %&gt;% group_by(VIC_RACE_short, SUSP_RACE_short, CRIME_CAT, BORO_NM, Premise_Derived, VIC_Individual_Flag, VIC_AGE_GROUP, Inside_Outside) %&gt;% summarize(Freq = n()) vcd::mosaic(VIC_AGE_GROUP ~ BORO_NM + Inside_Outside, counts3, direction = c(&quot;v&quot;,&quot;v&quot;, &quot;h&quot;)) We see here that more crime occurs inside compared to outside (which aligns with the fact most crime is larceny/theft). What is interesting there does not seem to be a difference between boroughs or age groups. Even accounting for uneven age groups in the raw data, it still seems like 25-44 is the largest category. 4.2 Time Series Analysis To begin the analysis on time, first we would like to justify why we filtered the data for only crimes that took place in 2022. ts_year_all &lt;- df %&gt;% #filter(year(Incident_Date) &gt;= 2022) %&gt;% group_by(year(Incident_Date)) %&gt;% summarize(Complaint_Count = n() ) %&gt;% rename( Incident_Year = `year(Incident_Date)`) ggplot(ts_year_all, aes(x=Incident_Year, y=Complaint_Count )) + geom_line() + scale_y_continuous(label=comma) + scale_x_continuous(limits= c(2000,2023) ) + labs( title = &quot;Date of Incident (Reported in 2022)&quot;, x = &quot;Incident Year (raw data)&quot;, y = &quot;Number of Reports&quot;, ) From this we see that there is a large drop off in historic reports. This make sense as it is more likely that someone would report a crime in the same year that it occurs. Although NYPD allows people to report crimes that occurred in 2020, there will be much fewer of them reported in 2022. If we wanted to use data prior to 2022 we should include the old crime reports for prior years. Also there are some quality issues with historic data as we see very old crimes (year 1500) which indicate some human error or a record-keeping issue. So focusing on incidents in 2022, we can look at the overall trend during the year. ggplot(df_filter, aes(x=Incident_Month)) + geom_line(aes(fill=..count..),stat=&quot;bin&quot;,binwidth=1) + scale_x_continuous(limits = c(1,9), n.breaks=12) + scale_y_continuous(limits = c(0,50000), label=comma) + labs( title = &quot;2022 Crimes Per Month (Q1-Q3)&quot;, x = &quot;Month (as a number)&quot;, y = &quot;Total Reports&quot;, ) #TODO make graph prettier We can see there are more crimes reported in the summer months, presumably it is warmer. Let’s facet this: ggplot(df_filter, aes(x=Incident_Month, color= fct_infreq(CRIME_CAT) )) + geom_line(aes(fill=..count..),stat=&quot;bin&quot;,binwidth=1) + scale_x_continuous(limits = c(1,9), n.breaks=12) + scale_y_continuous(limits = c(0,20000), label=comma) + #facet_grid((CRIME_CAT) ~ .) + labs( title = &quot;2022 Crimes Per Month (Q1-Q3)&quot;, x = &quot;Month (as a number)&quot;, y = &quot;Total Reports&quot;, color = &quot;Crime Category&quot; ) Maybe, the trend is dominated by theft crimes but it does seem like there is a hump over summer. Uniquely there is a dip for drug crimes. 4.2.1 Day of Week Let’s now look at some other time related charts ggplot(df_filter, aes(y=CRIME_CAT , fill=fct_rev(Incident_DayOfWeek)) ) + geom_bar(stat=&#39;count&#39;, position=&#39;dodge&#39;) + scale_x_continuous(label=comma) + scale_fill_brewer(palette=&#39;Set2&#39;, breaks=c(&#39;Sun&#39;,&#39;Mon&#39;,&#39;Tue&#39;,&#39;Wed&#39;,&#39;Thu&#39;,&#39;Fri&#39;,&#39;Sat&#39;))+ #scale_fill_discrete(breaks=c(&#39;Sun&#39;,&#39;Mon&#39;,&#39;Tue&#39;,&#39;Wed&#39;,&#39;Thu&#39;,&#39;Fri&#39;,&#39;Sat&#39;)) + labs( title = &quot;2022 Crimes By Day of Week (Q1-Q3)&quot;, x = &quot;Count&quot;, y = &quot;&quot;, fill = &quot;Day&quot; ) We see some interesting trends here - Theft is lower on weekends, gambling up on weekends, sex crimes high on Wednesdays surprisingly. 4.2.2 Time of Day Let’s also look at the time of the day: ggplot(df_filter, aes(x = Incident_HourTime, y=fct_rev(Incident_DayOfWeek), fill=fct_rev(Incident_DayOfWeek) )) + geom_density_ridges() + scale_x_continuous(limits = c(0,24), breaks = seq(0, 24, by = 1)) + scale_fill_brewer(palette=&#39;Set2&#39;, breaks=c(&#39;Sun&#39;,&#39;Mon&#39;,&#39;Tue&#39;,&#39;Wed&#39;,&#39;Thu&#39;,&#39;Fri&#39;,&#39;Sat&#39;))+ labs( title = &quot;Hourly Breakdown (Total Q1-Q3 2022)&quot;, x = &quot;Time of Day (24 Clock)&quot;, y = &quot;Day of Week&quot; ) + theme(legend.position=&quot;none&quot;) #+ theme_classic(18) The raw data is rounded to the nearest minute but there is likely binning happening with time reported on the nearest hour. There appears to be a peak at 12:00pm (12 of 24) which is likely a data quality issue in that reports may default to that time when not specified exactly. This also happens as 12:00am (0 of 24) though it is harder to see due to how the ridge plot extrapolates at the boundary. That said, this graph shows that crimes seems to peak at night around 6pm. But is this related to individual crimes and in/around parks? ggplot(df_filter, aes(x = Incident_HourTime, y=Premise_Derived )) + geom_density_ridges() + scale_x_continuous(limits = c(0,24), breaks = seq(0, 24, by = 1)) + facet_grid((VIC_Individual_Flag) ~ .) + labs( title = &quot;Crimes by Premise and Time for Individuals vs Entities&quot;, subtitle = &quot;Y = Indivuals, N = Entities/Businesses/NY State&quot;, x = &quot;Time of Day (24 Clock)&quot;, y = &quot;Premise Category&quot; ) #TODO Show counts for each of these groups since not representative (ex what is a crime against and entity in a park)? Refer back to moaisic plot From prior charts, we know that the number of crimes is skewed toward those occurring in “Inside”. This does not represent the magnitude but we can see that for individuals, parks are more ‘dangerous’ at night - more so than other locations. However, it is somewhat comforting to know that this starts to drop off quickly after about 7-8pm. 4.3 Location Analysis 4.3.1 Spatial Distribution Around Columbia Since a component of our research is to analyze how crime is spread out around Columbia University, a visual analysis of spatial distribution of all crime around Columbia is useful. The following is a map centered around Columbia University (CU) that shows where crime is distributed. # Plottting columbia data on a map columbia &lt;- df_filter[df_filter$dist_to_CU &lt;= 2000,] columbia &lt;- columbia[!is.na(columbia$dist_to_CU),] #there are 9 rows with missing lat/long info #library(ggmap) register_google(key = &quot;AIzaSyDUVtOF3qSoJEcQAZ9mU0E1rGKQDqcuNGw&quot;) map = get_googlemap(center = c(lon = -73.963036, lat = 40.807384),zoom = 15) ggmap(map) + geom_point(aes(x = Longitude, y = Latitude), data = columbia, , color = &#39;red&#39;, position=position_jitter(h=0.0001, w=0.0001), alpha = 0.05, size = 1) + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank(), axis.title.x=element_blank(), axis.title.y=element_blank()) + ggtitle(&quot;Spatial distribution of crime around Columbia University&quot;) - The above map is useful to see how geographical location of a crime is coded in the dataset - for example, all crimes happening on a particular street (or residences on a particular street) between two avenues is assigned to the lat-long coordinates of the center of that street (See W 111th St to W 124th St for instance). Another thing this shows is around which subway station there is more crime. For example, out of the 2 125th st stations, the one on the right has noticeably more crime concentrated around it than the one on the left. Note that the way data is recorded, all crimes that happen inside a subway station are coded to the location of the subway station. So this result could also mean that one station is safer than the other. This map could also be used to plan one’s route to and from Columbia depending if safety is a concern. For example, one could see which streets and avenues have less crime concentrated on them and so could indicate which routes are generally safer to travel on. # Map of different crime types around Columbia map = get_googlemap(center = c(lon = -73.963036, lat = 40.807384),zoom = 15) ggmap(map) + geom_point(aes(x = Longitude, y = Latitude), data = columbia, , color = &#39;red&#39;, position=position_jitter(h=0.0001, w=0.0001), alpha = 0.05, size = 1) + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank(), axis.title.x=element_blank(), axis.title.y=element_blank()) + facet_wrap(~CRIME_CAT) 4.3.2 Increasing Radius from Columbia The following histogram was produced to check how the crime frequency increases with increasing radius away from school. # Line graph of density at different points ggplot(columbia, aes(dist_to_CU)) + geom_histogram(color = &quot;blue&quot;, fill = &quot;lightBlue&quot;, binwidth = 100) + scale_x_continuous(label=comma) + scale_y_continuous(label=comma) + labs( title = &quot;Histogram of Crime by Distance to Columbia&quot;, x = &quot;Distance to CU (meters)&quot;, y = &quot;Count&quot;, ) This graph shows that within each 100m radius (up to 2000m away from Columbia), the crime frequency increases. The explanation is that because the increase in area considered within each 100m bin increases polynomially (with the power of 2), it’s only natural that we see more crimes in buckets farther away from Columbia than those closer to columbia because the area considered within each bucket increases more than linearly. It is still interesting to note however that for some buckets the crime frequency goes down even though the area considered goes up. The next plots further investigate this issue using the concept of density of crime. 4.3.3 Cumulative Crime Near CU The previous plot motivated us to try to confirm whether the crime frequency follows a quadratic relationship with respect to radius from Columbia University. Note that this would imply that crime scales linearly with the area considered. # Line graph of density at different points away from Columbia #library(tidyverse) #columbia[, Radius := cut(dist_to_CU, breaks = c(0, 101, 201, 301, 401, 501, 601, 701, 801, 901, 1001, 1101, 1201, 1301, 1401, 1501, 1601, 1701, 1801, 1901, 2001), labels = c(&quot;0-100&quot;, &quot;100-200&quot;, &quot;3001-400&quot;, &quot;10001-20000&quot;))] ggplot(columbia, aes(dist_to_CU)) + stat_ecdf(color = &quot;blue&quot;, fill = &quot;lightBlue&quot;, binwidth = 100) + scale_x_continuous(label=comma) + scale_y_continuous(label=percent) + labs( title = &quot;Crime by Distance to Columbia (Cumulative)&quot;, x = &quot;Distance to CU (meters)&quot;, y = &quot;Crime Reports (% of total within 2000m)&quot;, ) Note that the curve roughly follows the shape of the quadratic curve confirming that the crime does increase linearly with area and quadratically with the radius considered. So then, what explains the decline in crime frequency observed in some of the buckets in the previous histogram? (For instance, the bucket that shows the crime frequency to drop between 1100m and 1200m as compared to the bucket that shows the crime frequency within 1000m and 1100m). We further explore this issue in our next plots. 4.3.4 Crime Density Since in the earlier histogram we saw that within some buckets the crime count was decreasing, we wanted to see how the density of crime changes as move away from Columbia generally and also specifically as we move from a bucket of 100m radius to the next bucket of the incremental 100m radius. # Making dataset to help plot density graphs #Your code factordist &lt;- factor(cut(columbia$dist_to_CU, breaks=c(0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100))) #Tabulate and turn into data.frame xout &lt;- as.data.frame(table(factordist)) #Add cumFreq and proportions xout &lt;- transform(xout, cumFreq = cumsum(Freq)) xout$factordist &lt;- as.numeric(xout$factordist) density &lt;- xout %&gt;% mutate(dist = case_when(factordist==1 ~ 100, factordist==2 ~ 200, factordist==3 ~ 300, factordist==4 ~ 400, factordist==5 ~ 500, factordist==6 ~ 600, factordist==7 ~ 700, factordist==8 ~ 800, factordist==9 ~ 900, factordist==10 ~ 1000, factordist==11 ~ 1100, factordist==12 ~ 1200, factordist==13 ~ 1300, factordist==14 ~ 1400, factordist==15 ~ 1500, factordist==16 ~ 1600, factordist==17 ~ 1700, factordist==18 ~ 1800, factordist==19 ~ 1900, factordist==20 ~ 2000)) density$cum_density &lt;- density$cumFreq/(density$dist * density$dist) density &lt;- density %&gt;% mutate(incremental_area = dist*dist - lag(dist*dist, default = first(dist*dist))) density$incremental_area[density$incremental_area == 0] &lt;- 100*100 density$inc_density &lt;- density$Freq/(density$incremental_area) # Making density graphs ggplot(density, aes(dist, cum_density)) + geom_line() + scale_x_continuous(label=comma) + labs( title = &quot;Cumulative Crime Density&quot;, x = &quot;Distance to CU (meters)&quot;, y = &quot;Density of Crime (# reports/m^2)&quot;, ) ggplot(density, aes(dist, inc_density)) + geom_line() + scale_x_continuous(label=comma) + labs( title = &quot;Incremental Crime Density&quot;, x = &quot;Distance to CU (meters)&quot;, y = &quot;Density of Crime (# reports/m^2)&quot;, ) Note that the cumulative density plot shows that crime is less dense within 750m of Columbia after which it becomes more dense. Potential explanations are that there could be extra policing around the campus area, or an increase in crime density due to an increase in population density as we move beyond 750m from Columbia. The incremental density plot shows the increase (or decrease) in density as we move from a bucket of 100m to radius to the bucket of next 100m radius around Columbia. Note that this graph also shows what the earlier histogram showed, i.e., for some buckets the density of crime goes down. # The following function uses the approximation that each degree of latitude represents 40075 / 360 kilometers and that each degree of longitude represents (40075 / 360) * cos(latitude) kilomemters. circles_data &lt;- function(centers, radius, nPoints = 100){ # centers: the data frame that has the lat/long coordinate of our point of interest # radius: radius measured in kilometer # nPoints: Defines the number of points on the circumference of the circle. The more the number of points, the smoother your circle will be meanLat &lt;- mean(centers$latitude) # length per longitude changes with lattitude, so need correction radiusLon &lt;- radius /111 / cos(meanLat/57.3) radiusLat &lt;- radius / 111 circleDF &lt;- data.frame(ID = rep(centers$ID, each = nPoints)) angle &lt;- seq(0,2*pi,length.out = nPoints) circleDF$lon &lt;- unlist(lapply(centers$longitude, function(x) x + radiusLon * cos(angle))) circleDF$lat &lt;- unlist(lapply(centers$latitude, function(x) x + radiusLat * sin(angle))) return(circleDF) } lat_long_data &lt;- data.frame(ID = c(&#39;1&#39;), latitude = c(40.807384),longitude = c(-73.963036)) ##ADDED THIS SO NO ERROR WOULD BE THROWN, IS THIS RIGHT?? circle_100m &lt;- circles_data(lat_long_data, 0.1) circle_200m &lt;- circles_data(lat_long_data, 0.2) circle_300m &lt;- circles_data(lat_long_data, 0.3) circle_400m &lt;- circles_data(lat_long_data, 0.4) circle_500m &lt;- circles_data(lat_long_data, 0.5) circle_600m &lt;- circles_data(lat_long_data, 0.6) circle_700m &lt;- circles_data(lat_long_data, 0.7) circle_800m &lt;- circles_data(lat_long_data, 0.8) circle_900m &lt;- circles_data(lat_long_data, 0.9) circle_1000m &lt;- circles_data(lat_long_data, 1) 4.3.5 Deeper Dive of Crime Density To finally see why we see buckets of incremental 100m radius away from Columbia with decreasing density of crime, we decided to plot these buckets visually on a google map of the area around Columbia overlapped with the spatial density of crime. map = get_googlemap(center = c(lon = -73.963036, lat = 40.807384),zoom = 15) ggmap(map) + geom_point(aes(x = Longitude, y = Latitude), data = df_filter, , color = &#39;red&#39;, position=position_jitter(h=0.0001, w=0.0001), alpha = 0.1, size = 1) + geom_polygon(data = circle_100m, aes(lon, lat), color = &quot;red&quot;, alpha = 0) + geom_polygon(data = circle_200m, aes(lon, lat), color = &quot;red&quot;, alpha = 0) + geom_polygon(data = circle_300m, aes(lon, lat), color = &quot;red&quot;, alpha = 0) + geom_polygon(data = circle_400m, aes(lon, lat), color = &quot;red&quot;, alpha = 0) + geom_polygon(data = circle_500m, aes(lon, lat), color = &quot;red&quot;, alpha = 0) + geom_polygon(data = circle_600m, aes(lon, lat), color = &quot;red&quot;, alpha = 0) + geom_polygon(data = circle_700m, aes(lon, lat), color = &quot;red&quot;, alpha = 0) + geom_polygon(data = circle_800m, aes(lon, lat), color = &quot;red&quot;, alpha = 0) + geom_polygon(data = circle_900m, aes(lon, lat), color = &quot;red&quot;, alpha = 0) + geom_polygon(data = circle_1000m, aes(lon, lat), color = &quot;red&quot;, alpha = 0) + theme(axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank(), axis.title.x=element_blank(), axis.title.y=element_blank()) + ggtitle(&quot;Spatial distribution of crime wrt incremental radius of 100m away from Columbia&quot;) We speculate that we see up and down jumps in incremental crime density due to the following reasons: While we increase the area in a circular fashion as we move away from Columbia, the lat/long coordinates of crime are recorded with the grid structure of Manhattan in mind. Therefore, some incremental radius/area buckets circles end up having way less crime reported if they contain parts of a park in them (since the crimes within parks are reported as happening on the streets outside the parks). Another reason is that many crimes are just reported to the center of the streets (as discussed in the spatial distribution map earlier). What ends up happening is that some incremental radius/area buckets end up including a straight line of all these concentrated crimes which lead to their crime density shooting up- see for example the area between the third last and the second last circle that ends up having majority of the crimes reported on streets W 112th to W 120th. 4.4 Various Other Plots (To Revise/Delete) –ATTEMPT AT MOSAIC PLOT –perhaps we can rename some values as a new field in the data.Rmd so that this shows a little cleaner? –try mixing and matching more fields in different moasics to see if anything pops? Open to other ideas but pretty sure we need a graph with 3+ categorical vars given this dataset. Try mosaic pair plots too. –add other charts about borough or jurisdiction? Age group? Might need to try a bunch but we’ll only include the ones we think look interesting (ie. where there is a pattern or anomaly/skew or something we can tie to real life) Heat map version: –need to clean these columns? add garbage values to unknown "],["interactive-component.html", "Chapter 5 Interactive Component 5.1 Interactive Map of Crime Around Columbia", " Chapter 5 Interactive Component #Code to create CSV files for use in interactive data columbia_short &lt;- df_key_fields %&gt;% filter(dist_to_CU &lt;= 1000) %&gt;% select(Latitude, Longitude) columbia_short_1 &lt;- df_key_fields %&gt;% filter(dist_to_CU &lt;= 1000) %&gt;% filter(VIC_SEX == &quot;M&quot;) %&gt;% select(Latitude, Longitude) columbia_short_2 &lt;- df_key_fields %&gt;% filter(dist_to_CU &lt;= 1000) %&gt;% filter(VIC_SEX == &quot;F&quot;) %&gt;% select(Latitude, Longitude) #write.csv(columbia_short, &quot;./data/columbia_short.csv&quot;, row.names=FALSE) write.csv(columbia_short_1, &quot;./data/columbia_short_1.csv&quot;, row.names=FALSE) write.csv(columbia_short_2, &quot;./data/columbia_short_2.csv&quot;, row.names=FALSE) The following map contains a list of crimes near Columbia (within 1km). Each dot indicates a reported crime (data filtered per the steps described in the “Data” section). Please be patient while the data renders. 5.1 Interactive Map of Crime Around Columbia My Google Map #map{ height:400px; width:100%; } (Select an option below to filter points on the map) Victim Gender: Male Female "],["conclusion.html", "Chapter 6 Conclusion", " Chapter 6 Conclusion "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
