# Results

```{r setup_results, include=FALSE}
knitr::opts_chunk$set(echo= TRUE, warning = FALSE, message = FALSE)
```

```{r}
library(scales)
#library(vcdExtra)
library(ggplot2)
library(forcats)
library(dplyr)
library(Lock5withR)
library(tidyr)

```


## Planning Notes
--To delete
```{r}
#What Graphs Do We Want?

#Data Prep Graphs
#1) Missing data patterns by row (using redav library or mi library) --heatmap


#Demographic Related Chart
#1) Bar Chart - CRIME_CAT by VIC Gender and/or SUSPECT Gender
#2) Mosaic Plot - TBD (CRIME CAT, VIC Gender, and Premise, +)
#3) Count by Premise (or use as a cut)
#--see ideas below

#Time Series Charts
#1) Line Chart - Overall 2022 Trend by Count
#2) Bar Chart - CRIME CAT by Day of Week
#3) Density Plot - Count by Time of Day (maybe with facets)
#4) (TBD more CRIME CAT by Time or Premise by Time of Day/Park)
#5) Look at box plot by time of day?

#Maps & Distance Info
#1) Counts By Borough (bar chart) (consider faceting by another dimension that looks good)
#2) Map of count in Columbia Area
#3) Line Graph of Density At Diff Points (x is distance from CU and y is total counts)
#4) Heat Map/Choropleth - Count by Precinct Code (may require addl data? --look into if feasible)

#Other Potential
#x) Parallel Coordinates Plot - not enough continuous vars
#x) Stacked Bar Chart?
#x) Scatter Plot (of Distance from CU vs Time of Day?)
#x) Cleveland Dot Plot - TBD

```


## Crime category and Demographic analysis

We start our analysis of crime types by investigating which crime categories occur in what frequencies. We bucketed the numerous crimes that are listed into logical categories.

```{r}
agg_tbl <- df_key_fields%>% group_by(CRIME_CAT) %>% 
  summarise(total_count=n(),
            .groups = 'drop')
df_counts <- agg_tbl %>% as.data.frame()

ggplot(df_counts) + geom_bar(aes(y=CRIME_CAT, x=total_count), stat="identity",fill = "cornflowerblue")+
  ggtitle("Number of crimes per category ") +
  xlab("Count") +
  ylab("Category")
```
We observe that theft or burglary related crimes are the most common by far (more than 2x the next highest category). This category is followed by sex crimes, violent crimes, and fraud/gambling related crimes all of which are relatively close in number. The lowest  category by far is drug and alcohol related crimes, perhaps because the policing of these crimes has been relaxed over the last few years.


We can further investigate the data to see the top 15 kinds of crimes and what categories they belong to.
```{r}
df_key_fields$FCT_CRIME_CAT <- as.factor(df_key_fields$CRIME_CAT)

df_key_fields %>%
  group_by(OFNS_DESC,FCT_CRIME_CAT) %>%
  summarise(freq = n()) %>%
  arrange(desc(freq)) %>%
  filter(freq>2750) %>%
  ggplot(aes(x = freq, y = fct_reorder(OFNS_DESC, freq), color = FCT_CRIME_CAT)) +
  geom_segment(aes(yend = OFNS_DESC), xend = 0, colour = "grey50") +
  geom_point(size = 3, aes(color = FCT_CRIME_CAT)) +
  ggtitle("Top 10 Most Common Types of Crime") +
  xlab("Counts") +
  ylab("Type of Crime")
```

Here, we can see the crime incident that has happened the most frequently is “Petit Larceny”, a form of larceny in which the value of the property taken is generally less than $50. 

As expected, 5 of the top 10 crimes are theft or burglary related, which explains why the most common crime category reported is theft. Also, we only see one incident type corresponding to drug and alcohol related crimes.


We can start our demographic analysis by digging further into the race, age, and gender of both suspects and victims. 
Lets begin by diving into trends between the race of the suspects and victims.
```{r}
df_key_fields %>%
  group_by(SUSP_RACE) %>%
  summarise(freq = n()) %>%
  arrange(desc(freq)) %>%
  slice(1:7) %>%
  ggplot(aes(x=fct_reorder(SUSP_RACE,freq,.desc = TRUE),y=freq)) +
  geom_bar(stat = "identity",fill = "cornflowerblue") +
  scale_x_discrete(guide = guide_axis(n.dodge=3))+
  ggtitle("Suspect Race vs Total Count") +
  xlab("Race") +
  ylab("Count")
```

```{r}
df_key_fields %>%
  group_by(VIC_RACE) %>%
  summarise(freq = n()) %>%
  arrange(desc(freq)) %>%
  slice(1:6) %>%
  ggplot(aes(x=fct_reorder(VIC_RACE,freq,.desc = TRUE),y=freq)) +
  geom_bar(stat = "identity",fill = "cornflowerblue") +
  scale_x_discrete(guide = guide_axis(n.dodge=3))+
  ggtitle("Victim Race vs Total Count") +
  xlab("Race") +
  ylab("Count")
```
While there is a lot of null and unknown data, we notice that there are clear patterns between the race of the suspect and victim. Visualizing this using a heatmap convinces us of this.  

```{r}
ggplot(df_key_fields, aes(VIC_RACE, SUSP_RACE)) + 
  geom_bin2d()+
  scale_fill_viridis_c()+
  scale_x_discrete(guide = guide_axis(n.dodge=3)) +
  xlab("Suspect Race") +
  ylab("Victim Race") +
  ggtitle("Victim Race vs Suspect Race")

```

We see from the bar charts that most of the victims are black and also that most of the suspects are black. Logically, the intersection of these two fields is also the highest. 


Next, lets do a gender focused analysis.
```{r}
df_key_fields %>%
  group_by(SUSP_SEX) %>%
  summarise(freq = n()) %>%
  arrange(desc(freq)) %>%
  #slice(1:6) %>%
  ggplot(aes(x=fct_reorder(SUSP_SEX,freq,.desc = TRUE),y=freq)) +
  geom_bar(stat = "identity",fill = "cornflowerblue") +
  #scale_x_discrete(guide = guide_axis(n.dodge=3))+
  ggtitle("Suspect Sex vs Total Count") +
  xlab("Sex") +
  ylab("Count")
```

```{r}
df_key_fields %>%
  group_by(VIC_SEX) %>%
  summarise(freq = n()) %>%
  arrange(desc(freq)) %>%
  #slice(1:6) %>%
  ggplot(aes(x=fct_reorder(VIC_SEX,freq,.desc = TRUE),y=freq)) +
  geom_bar(stat = "identity",fill = "cornflowerblue") +
  #scale_x_discrete(guide = guide_axis(n.dodge=3))+
  ggtitle("Victim Sex vs Total Count") +
  xlab("Sex") +
  ylab("Count")
```
Most of the victims are female, followed closely by males. However, there is a large discrepancy in the counts of suspect genders. Males are by far the highest category, more than 4x the count for females.


```{r}
ggplot(df_key_fields, aes(SUSP_SEX,VIC_SEX)) + 
  geom_bin2d()+
  scale_x_discrete(guide = guide_axis(n.dodge=3))

```


--ATTEMPT AT MOSAIC PLOT 
```{r}
counts2 <- df_key_fields %>%
  group_by(VIC_RACE, SUSP_RACE) %>%
  summarize(Freq = n())
vcd::mosaic(VIC_RACE ~ SUSP_RACE, counts2, direction = c("v", "h"),)
```
--perhaps we can rename some values as a new field in the data.Rmd so that this shows a little cleaner? 

--try mixing and matching more fields in different moasics to see if anything pops? Open to other ideas but pretty sure we need a graph with 3+ categorical vars given this dataset.  Try mosaic pair plots too.
```{r}
counts3 <- df_key_fields %>%
  group_by(VIC_RACE, SUSP_RACE, CRIME_CAT, BORO_NM, VIC_Individual_Flag) %>%
  summarize(Freq = n())
vcd::mosaic(VIC_Individual_Flag ~ BORO_NM + CRIME_CAT, counts3, direction = c("v","v","h"),)
vcd::mosaic(CRIME_CAT ~ BORO_NM + VIC_Individual_Flag, counts3, direction = c("v","v","h"),)
```

--add other charts about borough or jurisdiction? Age group? Might need to try a bunch but we'll only include the ones we think look interesting (ie. where there is a pattern or anomaly/skew or something we can tie to real life)
```{r}
df_key_fields %>%
  group_by(VIC_AGE_GROUP) %>%
  summarise(freq = n()) %>%
  arrange(desc(freq)) %>%
  slice(1:10) %>%
  ggplot(aes(x=fct_reorder(VIC_AGE_GROUP,freq,.desc = TRUE),y=freq)) +
  geom_bar(stat = "identity",fill = "cornflowerblue") +
  scale_x_discrete(guide = guide_axis(n.dodge=3))+
  ggtitle("Victim Age vs Total Count") +
  xlab("Age") +
  ylab("Count")
```

```{r}
df_key_fields %>%
  group_by(SUSP_AGE_GROUP) %>%
  summarise(freq = n()) %>%
  arrange(desc(freq)) %>%
  slice(1:10) %>%
  ggplot(aes(x=fct_reorder(SUSP_AGE_GROUP,freq,.desc = TRUE),y=freq)) +
  geom_bar(stat = "identity",fill = "cornflowerblue") +
  scale_x_discrete(guide = guide_axis(n.dodge=3))+
  ggtitle("Suspect Age vs Total Count") +
  xlab("Age") +
  ylab("Count")
```

Heat map version:
--need to clean these columns? add garbage values to unknown
```{r}
ggplot(df_key_fields, aes(VIC_AGE_GROUP, SUSP_AGE_GROUP)) + 
  geom_bin2d()+
  scale_fill_viridis_c()
```


## Time Series
To begin the analysis on time, first we would like to justify why we filtered the data for only crimes that took place in 2022. 
```{r}
ts_year_all <- df %>% 
  #filter(year(Incident_Date) >= 2022) %>%
  group_by(year(Incident_Date))  %>%
  summarize(Complaint_Count = n() ) %>% rename( Incident_Year = `year(Incident_Date)`)

ggplot(ts_year_all, aes(x=Incident_Year, y=Complaint_Count )) + geom_line() +
  scale_y_continuous(label=comma) +
  scale_x_continuous(limits= c(2000,2023) ) +
  labs(
    title = "Date of Incident (Reported in 2022)",
    x = "Incident Year (raw data)",
    y = "Number of Reports",
  )

```
From this we see that there is a large drop off in historic reports. This make sense as it is more likely that someone would report a crime in the same year that it occurs. Although NYPD allows people to report crimes that occurred in 2020, there will be much fewer of them reported in 2022. If we wanted to use data prior to 2022 we should include the old crime reports for prior years. Also there are some quality issues with historic data as we see very old crimes (year 1500) which indicate some human error or a record-keeping issue.  

So focusing on incidents in 2022, we can look at the overall trend during the year.
```{r}
ggplot(df_filter, aes(x=Incident_Month)) +
      geom_line(aes(fill=..count..),stat="bin",binwidth=1) +
      scale_x_continuous(limits = c(1,9), n.breaks=12) +
      scale_y_continuous(limits = c(0,50000), label=comma) +
      labs(
        title = "2022 Crimes Per Month (Q1-Q3)",
        x = "Month (as a number)",
        y = "Total Reports",
      )
#TODO make graph prettier
```
We can see there are more crimes reported in the summer months, presumably it is warmer. Let's facet this:

```{r}
ggplot(df_filter, aes(x=Incident_Month, color=
                        fct_infreq(CRIME_CAT)
                      )) +
      geom_line(aes(fill=..count..),stat="bin",binwidth=1) +
      scale_x_continuous(limits = c(1,9), n.breaks=12) +
      scale_y_continuous(limits = c(0,20000), label=comma) +
      #facet_grid((CRIME_CAT) ~ .) +
      labs(
        title = "2022 Crimes Per Month (Q1-Q3)",
        x = "Month (as a number)",
        y = "Total Reports",
        color = "Crime Category"
      ) 
```
Maybe, the trend is dominated by theft crimes but it does seem like there is a hump over summer. Uniquely there is a dip for drug crimes. 

Let's now look at some other time related charts
```{r}
#ggplot(df_filter, aes(x = Incident_DayOfWeek, fill=fct_infreq(CRIME_CAT))) + 
#  geom_bar(stat='count', position='dodge')

ggplot(df_filter, aes(y=CRIME_CAT , fill=fct_rev(Incident_DayOfWeek)) ) + 
  geom_bar(stat='count', position='dodge') +
  scale_x_continuous(label=comma) +
  scale_fill_discrete(breaks=c('Sun','Mon','Tue','Wed','Thu','Fri','Sat')) +
  labs(
      title = "2022 Crimes By Day of Week (Q1-Q3)",
      x = "Total Reports",
      y = "",
      fill = "Day"
      ) 
```
We see some interesting trends here - Theft is lower on weekends, gambling up on weekends, sex crimes high on Wednesdays surprisingly.

Let's also look at the time of the day:
```{r}
ggplot(df_filter, aes(x = Incident_HourTime, y=fct_rev(Incident_DayOfWeek) )) +
  geom_density_ridges() +
  scale_x_continuous(limits = c(0,24), breaks = seq(0, 24, by = 1)) +
  labs(
      title = "Hourly Breakdown (Total Q1-Q3 2022)",
      x = "Time of Day (24 Clock)",
      y = "Day of Week"
      ) #+ theme_classic(18)
#TODO change theme to look better?
#not quite sure why hour 24 does not equal hour 0...
```
The raw data is rounded to the nearest minute but there is likely binning happening with time  reported on the nearest hour. There also appears to be a peak at 12:00 which is likely a data quality issue in that reports may default to that time when not specified exactly. Otherwise this graph shows that crimes seems to peak at night around 6pm. But is this related to individual crimes and in/around parks?

```{r}
ggplot(df_filter, aes(x = Incident_HourTime, y=Premise_Derived )) +
  geom_density_ridges() +
  scale_x_continuous(limits = c(0,24), breaks = seq(0, 24, by = 1)) +
  facet_grid((VIC_Individual_Flag) ~ .) +
  labs(
      title = "Crimes by Premise and Time",
      subtitle = "Y = Indivuals, N = Entities/Businesses/NY State",
      x = "Time of Day (24 Clock)",
      y = "Premise Category"
      )
#TODO Show counts for each of these groups since not representative (ex what is a crime against and entity in a park)? Refer back to moaisic plot
```
From prior charts, we know that the number of crimes is skewed toward those occurring in "Inside". This does not represent the magnitude but we can see that for individuals, parks are more 'dangerous' at night - more so than other locations. However, it is somewhat comforting to know that this starts to drop off quickly after about 7-8pm. 


```{r}
#--Let's specifically look at crime around Columbia for Individuals

#ts_filter <- df_filter %>% filter(VIC_Individual_Flag == 'Y') %>% 
#  filter(dist_to_CU <= 2000) %>% #only 10k rows
#  filter(Premise_Derived == "PARK") #only 73 rows, not helpful

#--Box plot by time of day? TBD
```







# Location Analysis

```{r}
# Plottting columbia data on a map
library(ggmap)
columbia <- df_filter[df_filter$dist_to_CU <= 2000,]

register_google(key = "AIzaSyDUVtOF3qSoJEcQAZ9mU0E1rGKQDqcuNGw")
map = get_googlemap(center = c(lon = -73.963036, lat = 40.807384),zoom = 15)
ggmap(map) + geom_point(aes(x = Longitude, y = Latitude), data = columbia, , color = 'red', position=position_jitter(h=0.0001, w=0.0001), alpha = 0.05, size = 1)
```

```{r}
library(gridExtra)
# Displaying all the crime categories within 2 kilometers of columbia
columbia_vc <- columbia[columbia$CRIME_CAT == "MAJOR VIOLENT CRIMES",]
columbia_tb <- columbia[columbia$CRIME_CAT == "THEFT OR BURGLARY",]
columbia_fg <- columbia[columbia$CRIME_CAT == "FRAUD/GAMBLING AND MISC",]
columbia_sc <- columbia[columbia$CRIME_CAT == "SEX CRIMES",]
columbia_da <- columbia[columbia$CRIME_CAT == "DRUG AND ALCOHOL RELATEDS",]

vc_map <- ggmap(map) + geom_point(aes(x = Longitude, y = Latitude), data = columbia_vc, , color = 'red', position=position_jitter(h=0.0001, w=0.0001), alpha = 0.1, size = 1)

tb_map <- ggmap(map) + geom_point(aes(x = Longitude, y = Latitude), data = columbia_tb, , color = 'red', position=position_jitter(h=0.0001, w=0.0001), alpha = 0.05, size = 1)

fg_map <- ggmap(map) + geom_point(aes(x = Longitude, y = Latitude), data = columbia_fg, , color = 'red', position=position_jitter(h=0.0001, w=0.0001), alpha = 0.05, size = 1)

sc_map <- ggmap(map) + geom_point(aes(x = Longitude, y = Latitude), data = columbia_sc, , color = 'red', position=position_jitter(h=0.0001, w=0.0001), alpha = 0.05, size = 1)

da_map <- ggmap(map) + geom_point(aes(x = Longitude, y = Latitude), data = columbia_da, , color = 'red', position=position_jitter(h=0.0001, w=0.0001), alpha = 0.05, size = 1)

#grid.arrange(vc_map,tb_map,fg_map,sc_map, da_map, ncol=1)
#ggmap(map, base_layer = ggplot(aes(x = Longitude, y = Latitude), data = columbia, , color = 'red', position=position_jitter(h=0.0001, w=0.0001), size = 1))
```

```{r}
# Map of different crime types around Columbia

map = get_googlemap(center = c(lon = -73.963036, lat = 40.807384),zoom = 15)
ggmap(map) + geom_point(aes(x = Longitude, y = Latitude), data = columbia, , color = 'red', position=position_jitter(h=0.0001, w=0.0001), alpha = 0.05, size = 1) + facet_wrap(~CRIME_CAT)
```

```{r}
# Line graph of density at different points

ggplot(columbia, aes(dist_to_CU)) +
  ggtitle("Histogram of crime counts wrt to distance from columbia") +
  geom_histogram(color = "blue", fill = "lightBlue", binwidth = 100)
```

```{r}
# Line graph of density at different points away from Columbia
library(tidyverse)
#columbia[, Radius := cut(dist_to_CU, breaks = c(0, 101, 201, 301, 401, 501, 601, 701, 801, 901, 1001, 1101, 1201, 1301, 1401, 1501, 1601, 1701, 1801, 1901, 2001), labels = c("0-100", "100-200", "3001-400", "10001-20000"))]

ggplot(columbia, aes(dist_to_CU)) +
  ggtitle("Histogram of cumulative crime counts wrt to distance from columbia") + stat_ecdf(color = "blue", fill = "lightBlue", binwidth = 100)


```

```{r}
# Making dataset to help plot density graphs

#Your code
factordist <- factor(cut(columbia$dist_to_CU, breaks=c(0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100)))
#Tabulate and turn into data.frame
xout <- as.data.frame(table(factordist))
#Add cumFreq and proportions
xout <- transform(xout, cumFreq = cumsum(Freq))
xout$factordist <- as.numeric(xout$factordist)

density <- xout %>% 
  mutate(dist = case_when(factordist==1 ~ 100,
                          factordist==2 ~ 200,
                          factordist==3 ~ 300,
                          factordist==4 ~ 400,
                          factordist==5 ~ 500,
                          factordist==6 ~ 600,
                          factordist==7 ~ 700,
                          factordist==8 ~ 800,
                          factordist==9 ~ 900,
                          factordist==10 ~ 1000,
                          factordist==11 ~ 1100,
                          factordist==12 ~ 1200,
                          factordist==13 ~ 1300,
                          factordist==14 ~ 1400,
                          factordist==15 ~ 1500,
                          factordist==16 ~ 1600,
                          factordist==17 ~ 1700,
                          factordist==18 ~ 1800,
                          factordist==19 ~ 1900,
                          factordist==20 ~ 2000))

density$cum_density <- density$cumFreq/(density$dist * density$dist)
density <- density %>%  mutate(incremental_area = dist*dist - lag(dist*dist, default = first(dist*dist)))
density$incremental_area[density$incremental_area == 0] <- 100*100

density$inc_density <- density$Freq/(density$incremental_area)

```

```{r}
# Making density graphs
cum_density_plt <- ggplot(density, aes(dist, cum_density)) + geom_line()

inc_density_plt <- ggplot(density, aes(dist, inc_density)) + geom_line()

cum_density_plt
inc_density_plt
```


1. Around Columbia, density drops - potentially cause of more policing or less reporting to NYPD. 

```{r}
# Counts by Boroughs

library(vcdExtra)
library(ggplot2)
library(forcats)
library(dplyr)


boroughs_cts <- ggplot(df_filter) +  geom_bar(aes(x=BORO_NM))

borough_cts_ctype <- ggplot(df_filter) +  geom_bar(aes(x=BORO_NM)) + facet_wrap(~CRIME_CAT)


boroughs_cts
borough_cts_ctype


```

```{r}
lat_long_data = data.frame( # Making a dataframe that has the lat/long coordinates of our point of interest. In this case, it is Times Square, NYC.
    ID = as.numeric(c(1:1)),
    longitude = as.numeric(c(-73.963036)),
    latitude = as.numeric(c(40.807384))
)

```

The following function uses the approximation that each degree of latitude represents 40075 / 360 kilometers and that each degree of longitude represents (40075 / 360) * cos(latitude) kilomemters. 

```{r}
circles_data <- function(centers, radius, nPoints = 100){
    # centers: the data frame that has the lat/long coordinate of our point of interest
    # radius: radius measured in kilometer
    # nPoints: Defines the number of points on the circumference of the circle. The more the number of points, the smoother your circle will be
    meanLat <- mean(centers$latitude)
    # length per longitude changes with lattitude, so need correction
    radiusLon <- radius /111 / cos(meanLat/57.3) 
    radiusLat <- radius / 111
    circleDF <- data.frame(ID = rep(centers$ID, each = nPoints))
    angle <- seq(0,2*pi,length.out = nPoints)
    circleDF$lon <- unlist(lapply(centers$longitude, function(x) x + radiusLon * cos(angle)))
    circleDF$lat <- unlist(lapply(centers$latitude, function(x) x + radiusLat * sin(angle)))
    return(circleDF)
}
```

Now making the data for two circles with 350 meter and 750 meter radii. 
```{r}
circle_100m <- circles_data(lat_long_data, 0.1)
circle_200m <- circles_data(lat_long_data, 0.2)
circle_300m <- circles_data(lat_long_data, 0.3)
circle_400m <- circles_data(lat_long_data, 0.4)
circle_500m <- circles_data(lat_long_data, 0.5)
circle_600m <- circles_data(lat_long_data, 0.6)
circle_700m <- circles_data(lat_long_data, 0.7)
circle_800m <- circles_data(lat_long_data, 0.8)
circle_900m <- circles_data(lat_long_data, 0.9)
circle_1000m <- circles_data(lat_long_data, 1)
```


```{r}
map = get_googlemap(center = c(lon = -73.963036, lat = 40.807384),zoom = 15)

ggmap(map) + geom_point(aes(x = Longitude, y = Latitude), data = df_filter, , color = 'red', position=position_jitter(h=0.0001, w=0.0001), alpha = 0.1, size = 1) + geom_polygon(data = circle_100m, aes(lon, lat), color = "red", alpha = 0) +  geom_polygon(data = circle_200m, aes(lon, lat), color = "red", alpha = 0) +  geom_polygon(data = circle_300m, aes(lon, lat), color = "red", alpha = 0) +  geom_polygon(data = circle_400m, aes(lon, lat), color = "red", alpha = 0) +  geom_polygon(data = circle_500m, aes(lon, lat), color = "red", alpha = 0) +  geom_polygon(data = circle_600m, aes(lon, lat), color = "red", alpha = 0) +  geom_polygon(data = circle_700m, aes(lon, lat), color = "red", alpha = 0) +  geom_polygon(data = circle_800m, aes(lon, lat), color = "red", alpha = 0) +  geom_polygon(data = circle_900m, aes(lon, lat), color = "red", alpha = 0) +  geom_polygon(data = circle_1000m, aes(lon, lat), color = "red", alpha = 0)
```




